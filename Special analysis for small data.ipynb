{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:43:36.391841Z",
     "start_time": "2020-01-02T01:43:36.383144Z"
    }
   },
   "outputs": [],
   "source": [
    "def loadCoreArticle(string):    \n",
    "    f = open('/home/lr/Downloads/conference_data/' + string + '.txt','r')\n",
    "    sample = f.readlines()\n",
    "\n",
    "    dict_finallis_keyword = eval(sample[0])\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    return dict_finallis_keyword\n",
    "\"\"\"\n",
    "tochi_core = loadCoreArticle('tochi_core')\n",
    "avi_core = loadCoreArticle('avi_core')\n",
    "icmi_core = loadCoreArticle('icmi_core')\n",
    "gi_core = loadCoreArticle('gi_core')\n",
    "chi_core = loadCoreArticle('chi_core')\n",
    "idc_core = loadCoreArticle('idc_core')\n",
    "lak_review = loadCoreArticle('lak_review')\n",
    "\"\"\"\n",
    "\n",
    "cof = ['assets','avi','chi','cscw','dis','gi','group','icmi','idc','its','iui','lak','ls','mobilechi','tochi','ubicomp','uist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:43:39.295882Z",
     "start_time": "2020-01-02T01:43:39.250123Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "title_list = []\n",
    "reference_list = {}\n",
    "for s in cof:\n",
    "    name = s + \"_core\"\n",
    "    \n",
    "    data = loadCoreArticle(name)\n",
    "    #print (len(data))\n",
    "    title_list.extend(list(data.keys()))\n",
    "    for i in data:\n",
    "        reference_list[i] = copy.deepcopy(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:43:39.616340Z",
     "start_time": "2020-01-02T01:43:39.610139Z"
    }
   },
   "outputs": [],
   "source": [
    "title_list[8] = \": designing a new class of computational toys\"\n",
    "reference_list[title_list[8]] = reference_list['curlybot: designing a new class of computational toys']\n",
    "title_list[16] = \": Haptic Feedback to Enhance Early Reading\"\n",
    "reference_list[title_list[16]] = reference_list['FeelSleeve: Haptic Feedback to Enhance Early Reading']\n",
    "title_list[32] = \": The Effects of Curriculum-Aligned Making on Children's Self-Identity\"\n",
    "reference_list[title_list[32]] = reference_list[\"I Make, Therefor I Am: The Effects of Curriculum-Aligned Making on Children's Self-Identity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:43:44.590731Z",
     "start_time": "2020-01-02T01:43:40.691524Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    }
   ],
   "source": [
    "#获取数据\n",
    "from elasticsearch import helpers\n",
    "import elasticsearch\n",
    "\n",
    "\n",
    "total_text = {}\n",
    "total_bibliometrics = {}\n",
    "total_index_term = {}\n",
    "total_reference = {}\n",
    "total_author = {}\n",
    "article_author = {}\n",
    "total_article = {}\n",
    "total_id = {}\n",
    "total_conf = {}\n",
    "\n",
    "ES_SERVERS = [{\n",
    "    # 'host': 'localhost',\n",
    "    'host': '127.0.0.1',\n",
    "    'port': 9200\n",
    "}]\n",
    "\n",
    "es_client = elasticsearch.Elasticsearch(\n",
    "    hosts=ES_SERVERS\n",
    ")\n",
    "\n",
    "es_result = helpers.scan(\n",
    "    client=es_client,\n",
    "    query={\n",
    "    \"query\": {\n",
    "        \"match_all\": {}\n",
    "    }\n",
    "},\n",
    "    scroll='5m',\n",
    "    index='conference_total',\n",
    "    #doc_type='conference_total',\n",
    "    # timeout='1m'\n",
    ")\n",
    "# print(len(es_result))\n",
    "\n",
    "#h_heads=[]\n",
    "cont=0\n",
    "for i in es_result:\n",
    "    data_year = i['_source']\n",
    "    h_head = i['_source']['h_head']\n",
    "    id_num = i['_id']\n",
    "    year = data_year['h_year']\n",
    "    if year not in total_article.keys():\n",
    "        total_article[year] = {'title':[], 'abstract':[]}\n",
    "    #if 'group' not in data_year.keys():\n",
    "        #print (h_head)\n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "           \n",
    "    author = data_year['author']\n",
    "    #reference = data_year['reference']\n",
    "    index_term = data_year['index_term']\n",
    "    bibliometrics = data_year['citition']\n",
    "    title = data_year['title']\n",
    "    abstract = data_year['abstract']\n",
    "    if title not in title_list:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    cont += 1 \n",
    "    total_id.setdefault(year,[]).append(id_num)\n",
    "    total_bibliometrics.setdefault(year,[]).append(bibliometrics)\n",
    "    total_reference.setdefault(year,[]).append(reference_list[title])\n",
    "    total_index_term.setdefault(year,[]).append(index_term)\n",
    "    total_article[year]['title'].append(title)\n",
    "    total_article[year]['abstract'].append(abstract)\n",
    "    article_author.setdefault(year,[]).append(author)\n",
    "    total_conf.setdefault(year,[]).append(h_head)\n",
    "\n",
    "    \n",
    "author_data = article_author\n",
    "print (cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 对比共同作者"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:47:46.791760Z",
     "start_time": "2020-01-02T01:47:46.696155Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#初始化列表\n",
    "num_same_author = {}\n",
    "single = {}\n",
    "for s in author_data:\n",
    "    #print (len(author_data[s]))\n",
    "    single[s] = [0] * len(author_data[s])\n",
    "#print (single)\n",
    "\n",
    "for s in author_data:\n",
    "    for i in author_data[s]:\n",
    "        num_same_author.setdefault(s,[]).append(copy.deepcopy(single))\n",
    "        \n",
    "#print (num_same_author)\n",
    "\n",
    "for s in author_data:\n",
    "    count = 0\n",
    "    for i in author_data[s]:\n",
    "        for j in i:\n",
    "            name1 = j['name']       \n",
    "            \n",
    "            for m in author_data:\n",
    "                temp = -1\n",
    "                for n in author_data[m]:\n",
    "                    temp += 1\n",
    "                    for o in n:\n",
    "                        name2 = o['name']\n",
    "                        \n",
    "                        if name1 == name2:\n",
    "                            num_same_author[s][count][m][temp] += 1\n",
    "                            \n",
    "                           \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 对比共同标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:47:49.572126Z",
     "start_time": "2020-01-02T01:47:49.486535Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#初始化列表\n",
    "num_same_index = {}\n",
    "single = {}\n",
    "for s in author_data:\n",
    "    #print (len(author_data[s]))\n",
    "    single[s] = [0] * len(author_data[s])\n",
    "#print (single)\n",
    "\n",
    "for s in author_data:\n",
    "    for i in author_data[s]:\n",
    "        num_same_index.setdefault(s,[]).append(copy.deepcopy(single))\n",
    "        \n",
    "for s in total_index_term:\n",
    "    count = 0\n",
    "    for i in total_index_term[s]:\n",
    "        #print (i)\n",
    "        for j in i:    \n",
    "            \n",
    "            for m in total_index_term:\n",
    "                temp = -1\n",
    "                for n in total_index_term[m]:\n",
    "                    temp += 1\n",
    "                    for o in n:                        \n",
    "                        if j == o:\n",
    "                            num_same_index[s][count][m][temp] += 1\n",
    "                            \n",
    "                           \n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 对比共同关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:47:52.309985Z",
     "start_time": "2020-01-02T01:47:51.374331Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "\n",
    "#设置stopwords，缩略词拆解\n",
    "\n",
    "stopwords = set (stopwords.words('english')+list(punctuation)+[').'])\n",
    "\n",
    "\n",
    "#分段成句和分句成词\n",
    "def splitSentence(paragraph):\n",
    "    sent = sent_tokenize(paragraph)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def wordtokenizer(sentence):\n",
    "    word = []\n",
    "    words = WordPunctTokenizer().tokenize(sentence)\n",
    "    word = word + words\n",
    "    return word\n",
    "\n",
    "#文章预处理\n",
    "def standardization(word_sent):\n",
    "#转换为小写字母    \n",
    "    text = []\n",
    "    for s in word_sent:\n",
    "        text.append(s.lower())\n",
    "    \n",
    "    return text\n",
    "\n",
    "#词形还原\n",
    "def lemmatizer(word):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = []\n",
    "#对单词类型标注\n",
    "    text_tag = np.array(nltk.pos_tag(word))\n",
    "    #print (text_tag)\n",
    "    \n",
    "    for s in text_tag:\n",
    "        if s[1].startswith('N'):\n",
    "            if s[1] == 'NNS' or s[1] == 'NN':\n",
    "                text.append(wnl.lemmatize(s[0],'n'))\n",
    "            else:\n",
    "                text.append(s[0])\n",
    "        elif s[1].startswith('V'):\n",
    "            text.append(wnl.lemmatize(s[0], 'v'))\n",
    "        elif s[1].startswith('J'):\n",
    "            text.append(wnl.lemmatize(s[0], 'a'))\n",
    "        elif s[1].startswith('R'):\n",
    "            text.append(wnl.lemmatize(s[0], 'r'))\n",
    "        else:\n",
    "            text.append(s[0])\n",
    "            \n",
    "    #print (text)\n",
    "    new_text = [word for word in text if word not in stopwords and 3<len(word)]\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "def filtrated(word):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    text = []\n",
    "#对单词类型标注\n",
    "    text_tag = np.array(nltk.pos_tag(word))\n",
    "    #print (text_tag)\n",
    "    \n",
    "    for s in text_tag:\n",
    "        if s[1].startswith('N'):            \n",
    "            text.append(s[0])\n",
    "        \n",
    "    \n",
    "    return text\n",
    "\n",
    "#统计词频\n",
    "def wordFreq(word):\n",
    "    word_Freq = {}\n",
    "    for s in word:\n",
    "        if not s in word_Freq:\n",
    "            word_Freq[s] = 1\n",
    "        else:\n",
    "            word_Freq[s] += 1\n",
    "    \n",
    "    num = 0\n",
    "    for key,value in word_Freq.items():\n",
    "        num = num + value\n",
    "    \n",
    "    return word_Freq, num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:47:54.955347Z",
     "start_time": "2020-01-02T01:47:52.311573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#提取关键词\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "education_keywords = ['education','learn','teach','educate','exam','student','curriculum','course']\n",
    "\n",
    "keyword_sum = {}\n",
    "for s in total_article:\n",
    "    for i in range(len(total_article[s]['title'])):\n",
    "        if total_article[s]['abstract'][i] != []:\n",
    "            text = total_article[s]['title'][i] + ' ' + total_article[s]['title'][i] + \" \" + total_article[s]['abstract'][i]\n",
    "        else:\n",
    "            text = total_article[s]['title'][i] + ' ' + total_article[s]['title'][i]\n",
    "            \n",
    "        sent = splitSentence(text) \n",
    "        \n",
    "        word_list = []\n",
    "        for m in sent:\n",
    "            word = wordtokenizer(m)                  \n",
    "            word_list = word_list + word\n",
    "        #print (word_list)\n",
    "        word_standard = standardization(word_list)\n",
    "        #print (word_standard)\n",
    "        word_ori = lemmatizer(word_standard)\n",
    "        \n",
    "        single = filtrated(word_ori)\n",
    "        word_Freq, num = wordFreq(single)\n",
    "        \n",
    "        temp = []\n",
    "        for j in word_Freq:\n",
    "            if word_Freq[j] > 1 and j not in education_keywords:\n",
    "                temp.append(j)\n",
    "        keyword_sum.setdefault(s,[]).append(temp)\n",
    "        #print (temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:47:55.115477Z",
     "start_time": "2020-01-02T01:47:54.957038Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#初始化列表\n",
    "num_same_keyword = {}\n",
    "single = {}\n",
    "for s in author_data:\n",
    "    #print (len(author_data[s]))\n",
    "    single[s] = [0] * len(author_data[s])\n",
    "#print (single)\n",
    "\n",
    "for s in author_data:\n",
    "    for i in author_data[s]:\n",
    "        num_same_keyword.setdefault(s,[]).append(copy.deepcopy(single))\n",
    "        \n",
    "for s in keyword_sum:\n",
    "    count = 0\n",
    "    for i in keyword_sum[s]:\n",
    "        for j in i:    \n",
    "            \n",
    "            for m in keyword_sum:\n",
    "                temp = -1\n",
    "                for n in keyword_sum[m]:\n",
    "                    temp += 1\n",
    "                    for o in n:                        \n",
    "                        if j == o:\n",
    "                            num_same_keyword[s][count][m][temp] += 1\n",
    "                            break\n",
    "                            \n",
    "                           \n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 对比共同参考文献"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-02T01:48:07.832911Z",
     "start_time": "2020-01-02T01:48:06.255935Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#初始化列表\n",
    "num_same_ref = {}\n",
    "single = {}\n",
    "for s in author_data:\n",
    "    #print (len(author_data[s]))\n",
    "    single[s] = [0] * len(author_data[s])\n",
    "#print (single)\n",
    "\n",
    "for s in author_data:\n",
    "    for i in author_data[s]:\n",
    "        num_same_ref.setdefault(s,[]).append(copy.deepcopy(single))\n",
    "        \n",
    "\n",
    "for s in total_reference:\n",
    "    count = 0\n",
    "    for i in total_reference[s]:\n",
    "        #print (i)\n",
    "        for j in i['reference']:    \n",
    "            \n",
    "            for m in total_reference:\n",
    "                temp = -1\n",
    "                for n in total_reference[m]:\n",
    "                    temp += 1\n",
    "                    for o in n['reference']:                        \n",
    "                        if j == o:\n",
    "                            num_same_ref[s][count][m][temp] += 1\n",
    "                            break\n",
    "                            \n",
    "                           \n",
    "        count += 1\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T11:02:56.175911Z",
     "start_time": "2019-12-26T11:02:56.163003Z"
    }
   },
   "source": [
    "# 计算相似得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:35:02.129663Z",
     "start_time": "2019-12-31T03:35:02.071628Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "#初始化列表\n",
    "sim_score = {}\n",
    "single = {}\n",
    "for s in author_data:\n",
    "    #print (len(author_data[s]))\n",
    "    single[s] = [0] * len(author_data[s])\n",
    "#print (single)\n",
    "\n",
    "for s in author_data:\n",
    "    for i in author_data[s]:\n",
    "        sim_score.setdefault(s,[]).append(copy.deepcopy(single))\n",
    "a = 5\n",
    "b = 5\n",
    "c = 0\n",
    "d = 0\n",
    "\n",
    "for s in total_article:\n",
    "    for i in range(len(total_article[s]['title'])):\n",
    "        \n",
    "        for year in num_same_author[s][i]:\n",
    "            for j in range(len(num_same_author[s][i][year])):\n",
    "                same_ref = num_same_ref[s][i][year][j]\n",
    "                same_author = num_same_author[s][i][year][j]\n",
    "                same_index = num_same_index[s][i][year][j]\n",
    "                same_keyword = num_same_keyword[s][i][year][j]\n",
    "                \n",
    "                sim_score[s][i][year][j] = a*same_ref + d*same_keyword + c*same_index + b*same_author  \n",
    "                \n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成力导向数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:35:04.465602Z",
     "start_time": "2019-12-31T03:35:04.255683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.DataFrame(columns = np.arange(164))\n",
    "article_position = {}\n",
    "num = 0\n",
    "for s in sim_score:\n",
    "    count = 0\n",
    "    for i in sim_score[s]:\n",
    "        title = total_article[s]['title'][count]\n",
    "        article_position[title] = num\n",
    "        temp = []\n",
    "        for j in i:\n",
    "            temp.extend(i[j])\n",
    "        #print (temp)\n",
    "        df.loc[num] = temp\n",
    "        \n",
    "        count += 1\n",
    "        num += 1\n",
    "#print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:35:04.860688Z",
     "start_time": "2019-12-31T03:35:04.655895Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -- 34 {weight: 3.0}\n",
      "5 -- 15 {weight: 1.5}\n",
      "5 -- 36 {weight: 1.5}\n",
      "5 -- 54 {weight: 1.5}\n",
      "6 -- 137 {weight: 1.5}\n",
      "6 -- 142 {weight: 1.5}\n",
      "11 -- 24 {weight: 2.0}\n",
      "11 -- 34 {weight: 1.5}\n",
      "11 -- 52 {weight: 2.5}\n",
      "11 -- 92 {weight: 2.0}\n",
      "11 -- 94 {weight: 1.5}\n",
      "11 -- 137 {weight: 1.5}\n",
      "11 -- 143 {weight: 3.5}\n",
      "14 -- 51 {weight: 2.5}\n",
      "15 -- 5 {weight: 1.5}\n",
      "15 -- 54 {weight: 1.5}\n",
      "17 -- 49 {weight: 1.5}\n",
      "17 -- 59 {weight: 1.5}\n",
      "17 -- 146 {weight: 1.5}\n",
      "21 -- 74 {weight: 1.5}\n",
      "23 -- 96 {weight: 2.0}\n",
      "24 -- 11 {weight: 2.0}\n",
      "24 -- 143 {weight: 1.5}\n",
      "31 -- 146 {weight: 1.5}\n",
      "31 -- 159 {weight: 1.5}\n",
      "34 -- 1 {weight: 3.0}\n",
      "34 -- 11 {weight: 1.5}\n",
      "34 -- 40 {weight: 1.5}\n",
      "34 -- 52 {weight: 1.5}\n",
      "34 -- 64 {weight: 1.5}\n",
      "36 -- 5 {weight: 1.5}\n",
      "36 -- 54 {weight: 1.5}\n",
      "36 -- 74 {weight: 1.5}\n",
      "36 -- 87 {weight: 2.0}\n",
      "37 -- 60 {weight: 1.5}\n",
      "40 -- 34 {weight: 1.5}\n",
      "45 -- 55 {weight: 1.5}\n",
      "45 -- 147 {weight: 1.5}\n",
      "47 -- 61 {weight: 2.0}\n",
      "47 -- 72 {weight: 2.5}\n",
      "47 -- 147 {weight: 2.5}\n",
      "49 -- 17 {weight: 1.5}\n",
      "49 -- 50 {weight: 3.5}\n",
      "49 -- 59 {weight: 1.5}\n",
      "49 -- 68 {weight: 1.5}\n",
      "49 -- 146 {weight: 1.5}\n",
      "49 -- 147 {weight: 2.0}\n",
      "50 -- 49 {weight: 3.5}\n",
      "50 -- 59 {weight: 1.5}\n",
      "51 -- 14 {weight: 2.5}\n",
      "51 -- 77 {weight: 1.5}\n",
      "52 -- 11 {weight: 2.5}\n",
      "52 -- 34 {weight: 1.5}\n",
      "52 -- 94 {weight: 1.5}\n",
      "54 -- 5 {weight: 1.5}\n",
      "54 -- 15 {weight: 1.5}\n",
      "54 -- 36 {weight: 1.5}\n",
      "54 -- 61 {weight: 1.5}\n",
      "54 -- 69 {weight: 1.5}\n",
      "54 -- 147 {weight: 1.5}\n",
      "55 -- 45 {weight: 1.5}\n",
      "56 -- 57 {weight: 2.0}\n",
      "57 -- 56 {weight: 2.0}\n",
      "59 -- 17 {weight: 1.5}\n",
      "59 -- 49 {weight: 1.5}\n",
      "59 -- 50 {weight: 1.5}\n",
      "59 -- 68 {weight: 2.0}\n",
      "59 -- 77 {weight: 3.0}\n",
      "59 -- 146 {weight: 2.0}\n",
      "60 -- 37 {weight: 1.5}\n",
      "61 -- 47 {weight: 1.5}\n",
      "61 -- 54 {weight: 1.5}\n",
      "61 -- 63 {weight: 1.5}\n",
      "61 -- 72 {weight: 2.0}\n",
      "61 -- 82 {weight: 2.5}\n",
      "61 -- 83 {weight: 2.0}\n",
      "61 -- 90 {weight: 1.5}\n",
      "62 -- 90 {weight: 1.5}\n",
      "63 -- 61 {weight: 1.5}\n",
      "63 -- 72 {weight: 1.5}\n",
      "64 -- 34 {weight: 1.5}\n",
      "64 -- 76 {weight: 7.0}\n",
      "64 -- 137 {weight: 2.0}\n",
      "65 -- 84 {weight: 5.5}\n",
      "67 -- 69 {weight: 1.5}\n",
      "68 -- 49 {weight: 1.5}\n",
      "68 -- 59 {weight: 2.0}\n",
      "68 -- 69 {weight: 3.0}\n",
      "68 -- 77 {weight: 8.0}\n",
      "69 -- 54 {weight: 1.5}\n",
      "69 -- 67 {weight: 1.5}\n",
      "69 -- 68 {weight: 3.0}\n",
      "72 -- 47 {weight: 2.5}\n",
      "72 -- 61 {weight: 2.0}\n",
      "72 -- 63 {weight: 1.5}\n",
      "72 -- 82 {weight: 3.5}\n",
      "74 -- 21 {weight: 1.5}\n",
      "74 -- 36 {weight: 1.5}\n",
      "76 -- 64 {weight: 7.0}\n",
      "76 -- 122 {weight: 2.0}\n",
      "76 -- 137 {weight: 1.5}\n",
      "77 -- 51 {weight: 1.5}\n",
      "77 -- 59 {weight: 3.0}\n",
      "77 -- 68 {weight: 8.0}\n",
      "77 -- 146 {weight: 2.0}\n",
      "82 -- 61 {weight: 3.0}\n",
      "82 -- 72 {weight: 3.5}\n",
      "83 -- 61 {weight: 2.0}\n",
      "84 -- 65 {weight: 5.5}\n",
      "85 -- 113 {weight: 3.5}\n",
      "87 -- 36 {weight: 2.0}\n",
      "88 -- 102 {weight: 1.5}\n",
      "90 -- 61 {weight: 1.5}\n",
      "90 -- 62 {weight: 1.5}\n",
      "92 -- 11 {weight: 2.0}\n",
      "94 -- 11 {weight: 1.5}\n",
      "94 -- 52 {weight: 1.5}\n",
      "95 -- 148 {weight: 1.5}\n",
      "96 -- 23 {weight: 2.0}\n",
      "96 -- 61 {weight: 1.5}\n",
      "98 -- 108 {weight: 2.0}\n",
      "98 -- 117 {weight: 2.5}\n",
      "98 -- 129 {weight: 3.5}\n",
      "102 -- 88 {weight: 1.5}\n",
      "108 -- 98 {weight: 1.5}\n",
      "108 -- 116 {weight: 7.0}\n",
      "108 -- 117 {weight: 1.5}\n",
      "108 -- 129 {weight: 1.5}\n",
      "111 -- 112 {weight: 1.5}\n",
      "112 -- 111 {weight: 1.5}\n",
      "113 -- 85 {weight: 3.5}\n",
      "116 -- 108 {weight: 7.0}\n",
      "116 -- 129 {weight: 1.5}\n",
      "116 -- 139 {weight: 1.5}\n",
      "117 -- 98 {weight: 2.5}\n",
      "117 -- 108 {weight: 1.5}\n",
      "118 -- 127 {weight: 2.0}\n",
      "122 -- 76 {weight: 2.0}\n",
      "122 -- 137 {weight: 1.5}\n",
      "125 -- 151 {weight: 2.5}\n",
      "127 -- 118 {weight: 2.0}\n",
      "129 -- 98 {weight: 3.5}\n",
      "129 -- 108 {weight: 2.0}\n",
      "129 -- 116 {weight: 1.5}\n",
      "129 -- 138 {weight: 1.5}\n",
      "129 -- 139 {weight: 1.5}\n",
      "134 -- 159 {weight: 2.0}\n",
      "135 -- 157 {weight: 2.0}\n",
      "137 -- 6 {weight: 1.5}\n",
      "137 -- 11 {weight: 1.5}\n",
      "137 -- 64 {weight: 2.0}\n",
      "137 -- 76 {weight: 1.5}\n",
      "137 -- 122 {weight: 1.5}\n",
      "137 -- 143 {weight: 2.5}\n",
      "137 -- 151 {weight: 2.0}\n",
      "137 -- 156 {weight: 2.0}\n",
      "138 -- 129 {weight: 1.5}\n",
      "138 -- 139 {weight: 3.0}\n",
      "139 -- 116 {weight: 1.5}\n",
      "139 -- 129 {weight: 1.5}\n",
      "139 -- 138 {weight: 3.0}\n",
      "140 -- 151 {weight: 1.5}\n",
      "140 -- 158 {weight: 2.0}\n",
      "142 -- 6 {weight: 1.5}\n",
      "143 -- 11 {weight: 3.5}\n",
      "143 -- 24 {weight: 1.5}\n",
      "143 -- 137 {weight: 2.5}\n",
      "144 -- 148 {weight: 1.5}\n",
      "146 -- 17 {weight: 1.5}\n",
      "146 -- 31 {weight: 1.5}\n",
      "146 -- 49 {weight: 1.5}\n",
      "146 -- 59 {weight: 2.0}\n",
      "146 -- 77 {weight: 2.0}\n",
      "147 -- 47 {weight: 2.0}\n",
      "147 -- 49 {weight: 1.5}\n",
      "147 -- 54 {weight: 1.5}\n",
      "148 -- 95 {weight: 1.5}\n",
      "148 -- 144 {weight: 1.5}\n",
      "151 -- 125 {weight: 2.5}\n",
      "151 -- 137 {weight: 2.0}\n",
      "151 -- 140 {weight: 1.5}\n",
      "151 -- 158 {weight: 3.0}\n",
      "156 -- 137 {weight: 2.0}\n",
      "157 -- 135 {weight: 2.0}\n",
      "157 -- 163 {weight: 1.5}\n",
      "158 -- 140 {weight: 2.0}\n",
      "158 -- 151 {weight: 3.0}\n",
      "159 -- 31 {weight: 1.5}\n",
      "159 -- 134 {weight: 2.0}\n",
      "163 -- 157 {weight: 1.5}\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for s in df:\n",
    "    #if s > 100:\n",
    "        #break\n",
    "    for i in df:\n",
    "        #if i > 100:\n",
    "            #break\n",
    "        if s != i and df[s][i] > 14:\n",
    "            print (str(s) + ' -- ' + str(i) + \" {weight: \" + str(df[s][i]/10) + \"}\")\n",
    "            t.append(s)\n",
    "            t.append(i)\n",
    "s2 = list(set(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-31T03:35:05.475979Z",
     "start_time": "2019-12-31T03:35:05.471128Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for s in range(164):\n",
    "    if s not in s2:\n",
    "        print (s)\"\"\"\n",
    "print (len(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T07:51:02.697866Z",
     "start_time": "2019-12-30T07:51:02.504133Z"
    }
   },
   "outputs": [],
   "source": [
    "t = []\n",
    "node_temp = {}\n",
    "edges_temp = []\n",
    "count = 0\n",
    "for s in df:\n",
    "    #if s > 100:\n",
    "        #break\n",
    "    for i in df:\n",
    "        #if i > 100:\n",
    "            #break\n",
    "        if s != i and df[s][i] > 10:\n",
    "            if s not in node_temp.keys():\n",
    "                node_temp[s] = count\n",
    "                count += 1\n",
    "            if i not in node_temp.keys():\n",
    "                node_temp[i] = count\n",
    "                count += 1\n",
    "            edges_temp.append({\"source\": s, \"target\": i, \"weight\": df[s][i]/2})\n",
    "                \n",
    "s2 = list(set(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T07:52:57.661740Z",
     "start_time": "2019-12-30T07:52:57.652117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name:': 11}, {'name:': 143}, {'name:': 64}, {'name:': 76}, {'name:': 65}, {'name:': 84}, {'name:': 68}, {'name:': 77}, {'name:': 85}, {'name:': 113}, {'name:': 108}, {'name:': 116}, {'name:': 138}, {'name:': 139}]\n",
      "[{'source': 0, 'target': 1, 'weight': 7.0}, {'source': 2, 'target': 3, 'weight': 11.0}, {'source': 4, 'target': 5, 'weight': 7.0}, {'source': 6, 'target': 7, 'weight': 12.0}, {'source': 3, 'target': 2, 'weight': 11.0}, {'source': 7, 'target': 6, 'weight': 12.0}, {'source': 5, 'target': 4, 'weight': 7.0}, {'source': 8, 'target': 9, 'weight': 6.0}, {'source': 10, 'target': 11, 'weight': 12.0}, {'source': 9, 'target': 8, 'weight': 6.0}, {'source': 11, 'target': 10, 'weight': 12.0}, {'source': 12, 'target': 13, 'weight': 6.0}, {'source': 13, 'target': 12, 'weight': 6.0}, {'source': 1, 'target': 0, 'weight': 7.0}]\n"
     ]
    }
   ],
   "source": [
    "node = []\n",
    "edge = []\n",
    "for s in node_temp:\n",
    "    node.append({\"name:\": s})\n",
    "    \n",
    "print (node)\n",
    "for s in edges_temp:\n",
    "    source = s[\"source\"]\n",
    "    target = s['target']\n",
    "    edge.append({\"source\": node_temp[source], \"target\": node_temp[target], \"weight\": s['weight']})\n",
    "    print (edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df[95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 修正引用数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T13:17:11.828695Z",
     "start_time": "2019-12-26T13:17:11.799612Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.666666666666664\n",
      "12.0\n",
      "29.333333333333332\n",
      "13.333333333333332\n",
      "12.0\n",
      "12.0\n",
      "21.333333333333332\n",
      "12.0\n",
      "13.333333333333332\n",
      "13.333333333333332\n",
      "12.0\n",
      "12.0\n",
      "13.333333333333332\n",
      "12.0\n",
      "12.0\n",
      "15.0\n",
      "1.0\n",
      "20.0\n",
      "13.0\n",
      "13.0\n",
      "18.0\n",
      "12.0\n",
      "13.0\n",
      "13.0\n",
      "25.0\n",
      "19.0\n",
      "14.0\n",
      "18.0\n",
      "12.0\n",
      "26.0\n",
      "12.0\n",
      "12.0\n",
      "13.0\n",
      "16.0\n",
      "22.0\n",
      "14.0\n",
      "24.0\n",
      "36.0\n",
      "12.8\n",
      "26.400000000000002\n",
      "15.200000000000001\n",
      "17.6\n",
      "11.200000000000001\n",
      "16.8\n",
      "11.200000000000001\n",
      "13.600000000000001\n",
      "17.6\n",
      "14.4\n",
      "14.4\n",
      "12.8\n",
      "11.200000000000001\n",
      "14.4\n",
      "34.4\n",
      "12.0\n",
      "21.6\n",
      "22.400000000000002\n",
      "13.600000000000001\n",
      "16.0\n",
      "12.8\n",
      "22.400000000000002\n",
      "24.0\n",
      "18.400000000000002\n",
      "19.200000000000003\n",
      "28.8\n",
      "11.200000000000001\n",
      "30.400000000000002\n",
      "12.666666666666666\n",
      "30.0\n",
      "33.33333333333333\n",
      "66.0\n",
      "12.666666666666666\n",
      "20.666666666666664\n",
      "16.0\n",
      "16.0\n",
      "14.666666666666666\n",
      "14.0\n",
      "11.333333333333332\n",
      "29.333333333333332\n",
      "12.0\n",
      "16.666666666666664\n",
      "16.0\n",
      "25.333333333333332\n",
      "25.333333333333332\n",
      "13.333333333333332\n",
      "88.66666666666666\n",
      "14.285714285714285\n",
      "10.857142857142856\n",
      "69.14285714285714\n",
      "31.428571428571427\n",
      "24.57142857142857\n",
      "16.0\n",
      "29.142857142857142\n",
      "14.857142857142856\n",
      "15.428571428571427\n",
      "14.285714285714285\n",
      "12.0\n",
      "44.57142857142857\n",
      "27.428571428571427\n",
      "10.857142857142856\n",
      "19.428571428571427\n",
      "15.5\n",
      "19.5\n",
      "22.5\n",
      "28.0\n",
      "34.5\n",
      "28.0\n",
      "15.5\n",
      "13.0\n",
      "15.0\n",
      "12.5\n",
      "12.5\n",
      "21.777777777777775\n",
      "16.0\n",
      "16.444444444444443\n",
      "18.666666666666664\n",
      "16.444444444444443\n",
      "16.444444444444443\n",
      "12.888888888888888\n",
      "11.666666666666666\n",
      "12.333333333333332\n",
      "10.666666666666666\n",
      "12.333333333333332\n",
      "13.0\n",
      "12.0\n",
      "11.142857142857142\n",
      "12.285714285714285\n",
      "12.8\n",
      "16.0\n",
      "19.200000000000003\n",
      "14.4\n",
      "10.5\n",
      "24.75\n",
      "24.0\n",
      "20.75\n",
      "17.25\n",
      "13.25\n",
      "10.545454545454545\n",
      "25.09090909090909\n",
      "19.272727272727273\n",
      "20.0\n",
      "14.4\n",
      "17.2\n",
      "14.0\n",
      "14.0\n",
      "14.0\n",
      "22.0\n",
      "16.0\n",
      "14.0\n",
      "14.0\n",
      "26.0\n",
      "16.266666666666666\n",
      "35.733333333333334\n",
      "10.526315789473683\n",
      "11.692307692307693\n",
      "24.30769230769231\n",
      "12.307692307692308\n",
      "45.53846153846154\n",
      "14.153846153846155\n",
      "25.09090909090909\n",
      "11.454545454545455\n",
      "19.652173913043477\n",
      "35.65217391304348\n",
      "13.91304347826087\n",
      "14.956521739130434\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "now = 2019\n",
    "index = 4\n",
    "high_cited = {}\n",
    "missing = {}\n",
    "new_cite_list = []\n",
    "for s in total_bibliometrics:\n",
    "    #print (s)\n",
    "    #print (total_bibliometrics[s])\n",
    "    count = 0\n",
    "    if s == \"2014/2015\":\n",
    "        year = 2014.0\n",
    "    else:\n",
    "        year = float(s)\n",
    "    for i in total_bibliometrics[s]:\n",
    "        #print (i)\n",
    "\n",
    "        new_cite = index / (now - year + 1) * (i + 1)\n",
    "        print (new_cite)\n",
    "        new_cite_list.append(new_cite)\n",
    "print (len(new_cite_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T13:35:05.098674Z",
     "start_time": "2019-12-26T13:35:05.094635Z"
    }
   },
   "outputs": [],
   "source": [
    "color = ['#97CBFF','#2894FF','#005AB5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T13:43:12.507376Z",
     "start_time": "2019-12-26T13:43:12.492518Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2{color:#97CBFF}\n",
      "6{color:#97CBFF}\n",
      "24{color:#97CBFF}\n",
      "29{color:#97CBFF}\n",
      "34{color:#97CBFF}\n",
      "36{color:#97CBFF}\n",
      "37{color:#2894FF}\n",
      "39{color:#97CBFF}\n",
      "52{color:#2894FF}\n",
      "54{color:#97CBFF}\n",
      "55{color:#97CBFF}\n",
      "59{color:#97CBFF}\n",
      "60{color:#97CBFF}\n",
      "63{color:#97CBFF}\n",
      "65{color:#2894FF}\n",
      "67{color:#97CBFF}\n",
      "68{color:#2894FF}\n",
      "69{color:#005AB5}\n",
      "71{color:#97CBFF}\n",
      "77{color:#97CBFF}\n",
      "81{color:#97CBFF}\n",
      "82{color:#97CBFF}\n",
      "84{color:#005AB5}\n",
      "87{color:#005AB5}\n",
      "88{color:#2894FF}\n",
      "89{color:#97CBFF}\n",
      "91{color:#97CBFF}\n",
      "96{color:#005AB5}\n",
      "97{color:#97CBFF}\n",
      "102{color:#97CBFF}\n",
      "103{color:#97CBFF}\n",
      "104{color:#2894FF}\n",
      "105{color:#97CBFF}\n",
      "111{color:#97CBFF}\n",
      "131{color:#97CBFF}\n",
      "132{color:#97CBFF}\n",
      "133{color:#97CBFF}\n",
      "137{color:#97CBFF}\n",
      "145{color:#97CBFF}\n",
      "149{color:#97CBFF}\n",
      "151{color:#2894FF}\n",
      "154{color:#97CBFF}\n",
      "156{color:#005AB5}\n",
      "158{color:#97CBFF}\n",
      "161{color:#2894FF}\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(new_cite_list)):\n",
    "    if new_cite_list[s] > 40:\n",
    "        print (str(s) + \"{color:\" + color[2] + \"}\")\n",
    "        continue\n",
    "    if new_cite_list[s] > 30:\n",
    "        print (str(s) + \"{color:\" + color[1] + \"}\")\n",
    "        continue\n",
    "    if new_cite_list[s] > 20:\n",
    "        print (str(s) + \"{color:\" + color[0] + \"}\")\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导出文章信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T13:03:07.094234Z",
     "start_time": "2019-12-26T13:03:07.082670Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.DataFrame(columns = ['index','title','abstract','author','citation','conference','year','id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T02:12:37.416426Z",
     "start_time": "2019-12-27T02:12:37.335858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    index                                              title  \\\n",
      "142   142  Blocks4All: Overcoming Accessibility Barriers ...   \n",
      "143   143  Bots & (Main)Frames: Exploring the Impact of T...   \n",
      "144   144  Science Everywhere: Designing Public, Tangible...   \n",
      "145   145  “Bursting the Assistance Bubble”: Designing In...   \n",
      "146   146  ConceptScape: Collaborative Concept Mapping fo...   \n",
      "147   147  Collaborative Live Media Curation: Shared Cont...   \n",
      "148   148  Co-designing online privacy-related games and ...   \n",
      "149   149  EmotiW 2018: Audio-Video, Student Engagement a...   \n",
      "0       0  Enabling Real-Time Adaptivity in MOOCs with a ...   \n",
      "1       1  Learning to Code in Localized Programming Lang...   \n",
      "2       2  Writing Reusable Code Feedback at Scale with M...   \n",
      "3       3  Intelligent tutors as teachers' aides: explori...   \n",
      "4       4  Trends and issues in student-facing learning a...   \n",
      "5       5  Follow the successful crowd: raising MOOC comp...   \n",
      "6       6  Enabling Collaboration in Learning Computer Pr...   \n",
      "7       7  : The Effects of Curriculum-Aligned Making on ...   \n",
      "8       8  A Social Media Based Index of Mental Well-Bein...   \n",
      "9       9  Why Tangibility Matters: A Design Case Study o...   \n",
      "10     10  Sidestepping the Elephant in the Classroom: Us...   \n",
      "11     11       Teaching Programming with Gamified Semantics   \n",
      "12     12  Wearable Immersive Virtual Reality for Childre...   \n",
      "13     13  \"They basically like destroyed the school one ...   \n",
      "14     14  Language learning on-the-go: opportune moments...   \n",
      "15     15  Effects of In-Video Quizzes on MOOC Lecture Vi...   \n",
      "16     16          Expert Evaluation of 300 Projects per Day   \n",
      "17     17  AXIS: Generating Explanations at Scale with Le...   \n",
      "18     18  Combining click-stream data with NLP tools to ...   \n",
      "19     19  Introduction of learning visualisations and me...   \n",
      "20     20  Privacy and analytics: it's a DELICATE issue a...   \n",
      "21     21  A conceptual framework linking learning design...   \n",
      "..    ...                                                ...   \n",
      "129   129  Digital mysteries: designing for learning at t...   \n",
      "130   130  Building and evaluating an intelligent pedagog...   \n",
      "131   131  Off-task behavior in the cognitive tutor class...   \n",
      "132   132  Ambient wood: designing new forms of digital a...   \n",
      "133   133                 Visualizing programs with Jeliot 3   \n",
      "134   134  Lessons learned from eClass: Assessing automat...   \n",
      "135   135  The introduction of a shared interactive surfa...   \n",
      "118   118  Designing e-learning games for rural children ...   \n",
      "119   119  Explore! possibilities and challenges of mobil...   \n",
      "120   120  Playful toothbrush: ubicomp technology for tea...   \n",
      "121   121  CareLog: a selective archiving tool for behavi...   \n",
      "122   122  Playing with the sound maker: do embodied meta...   \n",
      "160   160  Designing for or designing with? Informant des...   \n",
      "161   161  The persona effect: affective impact of animat...   \n",
      "162   162  KidPad: a design collaboration between childre...   \n",
      "163   163  The Effect of Turn-Taking Protocols on Childre...   \n",
      "150   150  Livenotes: a system for cooperative and augmen...   \n",
      "151   151  Extending tangible interfaces for education: d...   \n",
      "152   152  Locus of feedback control in computer-based tu...   \n",
      "136   136  Two peers are better than one: aggregating pee...   \n",
      "137   137  Comparing the use of tangible and graphical pr...   \n",
      "138   138  Tabletop displays for small group study: affor...   \n",
      "139   139  Children designing together on a multi-touch t...   \n",
      "153   153  Implicit coordination in firefighting practice...   \n",
      "154   154  The life and death of online gaming communitie...   \n",
      "155   155  Modeling and understanding students' off-task ...   \n",
      "156   156  Storytelling alice motivates middle school gir...   \n",
      "157   157  Multiple mice for retention tasks in disadvant...   \n",
      "158   158      Digital manipulatives: new toys to think with   \n",
      "159   159  Investigating the capture, integration and acc...   \n",
      "\n",
      "                                              abstract  \\\n",
      "142  Blocks-based programming environments are a po...   \n",
      "143  While recent work has begun to evaluate the ef...   \n",
      "144  A major challenge in education is understandin...   \n",
      "145  Children living with visual impairments (VIs) ...   \n",
      "146  While video has become a widely adopted medium...   \n",
      "147  In recent years, online education's reach and ...   \n",
      "148  Children ages 8--12 spend nearly six hours per...   \n",
      "149  This paper details the sixth Emotion Recogniti...   \n",
      "0    In this paper, we demonstrate a first-of-its-k...   \n",
      "1    Education research suggests that learning in o...   \n",
      "2    In large introductory programming classes, tea...   \n",
      "3    Intelligent tutoring systems (ITSs) are common...   \n",
      "4    We conducted a literature review on systems th...   \n",
      "5    Social comparison theory asserts that we estab...   \n",
      "6    We investigate how technology can support coll...   \n",
      "7    Prior research investigating the effects of in...   \n",
      "8    Psychological distress in the form of depressi...   \n",
      "9    Tangibles may be effective for reading applica...   \n",
      "10   Cultural taboos can restrict student learning ...   \n",
      "11   Dominant approaches to programming education e...   \n",
      "12   Our research explores the potential of Wearabl...   \n",
      "13   This exploratory work studies the effects of e...   \n",
      "14   Learning a foreign language is a daunting and ...   \n",
      "15   Online courses on sites such as Coursera use q...   \n",
      "16   In October 2014, one-time MOOC developer Udaci...   \n",
      "17   While explanations may help people learn by pr...   \n",
      "18   Completion rates for massive open online class...   \n",
      "19   This paper describes open learner models as vi...   \n",
      "20   The widespread adoption of Learning Analytics ...   \n",
      "21   In this paper we present a learning analytics ...   \n",
      "..                                                 ...   \n",
      "129  We present the iterative design, implementatio...   \n",
      "130  Electronic educational games can be highly ent...   \n",
      "131  We investigate the prevalence and learning imp...   \n",
      "132  Ubiquitous and mobile technologies provide opp...   \n",
      "133  We present a program visualization tool called...   \n",
      "134  This article presents results from a study of ...   \n",
      "135  We describe a user study of a large multi-user...   \n",
      "118  Poor literacy remains a barrier to economic em...   \n",
      "119  This paper reports the experimental studies we...   \n",
      "120  This case study in UbiComp technology and desi...   \n",
      "121  Identifying the function of problem behavior c...   \n",
      "122  In this paper we present the results of a comp...   \n",
      "160                      An abstract is not available.   \n",
      "161                      An abstract is not available.   \n",
      "162                      An abstract is not available.   \n",
      "163  This study compared the influence of turn-taki...   \n",
      "150  We describe Livenotes, a shared whiteboard sys...   \n",
      "151  This paper introduces a new framework for thin...   \n",
      "152  The advent of second-generation intelligent co...   \n",
      "136  Scientific peer review, open source software d...   \n",
      "137  Much of the work done in the field of tangible...   \n",
      "138  In this paper we compare the affordances of pr...   \n",
      "139  Applications running on multi-touch tabletops ...   \n",
      "153  Fire emergency response requires rapidly proce...   \n",
      "154  Massively multiplayer online games (MMOGs) can...   \n",
      "155  We present a machine-learned model that can au...   \n",
      "156  We describe Storytelling Alice, a programming ...   \n",
      "157  This study evaluates single-mouse and multiple...   \n",
      "158                      An abstract is not available.   \n",
      "159                      An abstract is not available.   \n",
      "\n",
      "                                                author citation conference  \\\n",
      "142  [{'name': 'Lauren R. Milne', 'institutions': '...        6        CHI   \n",
      "143  [{'name': 'Edward F. Melcer', 'institutions': ...        6        CHI   \n",
      "144  [{'name': 'June Ahn', 'institutions': 'New Yor...        6        CHI   \n",
      "145  [{'name': 'Oussama Metatla', 'institutions': '...       10        CHI   \n",
      "146  [{'name': 'Ching Liu', 'institutions': 'Nation...        7        CHI   \n",
      "147  [{'name': 'William A. Hamilton', 'institutions...        6        CHI   \n",
      "148  [{'name': 'Priya Kumar', 'institutions': 'Univ...        6        IDC   \n",
      "149  [{'name': 'Abhinav Dhall', 'institutions': 'In...       12       ICMI   \n",
      "0    [{'name': 'Zachary A. Pardos', 'institutions':...       13         LS   \n",
      "1    [{'name': 'Sayamindu Dasgupta', 'institutions'...        8         LS   \n",
      "2    [{'name': 'Andrew Head', 'institutions': 'Univ...       21         LS   \n",
      "3    [{'name': 'Kenneth Holstein', 'institutions': ...        9        LAK   \n",
      "4    [{'name': 'Robert Bodily', 'institutions': 'Br...        8        LAK   \n",
      "5    [{'name': 'Dan Davis', 'institutions': 'Delft ...        8        LAK   \n",
      "6    [{'name': 'Anja Thieme', 'institutions': 'Micr...       15        DIS   \n",
      "7    [{'name': 'Sharon Lynn Chu', 'institutions': '...        8        CHI   \n",
      "8    [{'name': 'Shrey Bagroy', 'institutions': 'III...        9        CHI   \n",
      "9    [{'name': 'Min Fan', 'institutions': 'Simon Fr...        9        CHI   \n",
      "10   [{'name': 'Piya Sorcar', 'institutions': 'Stan...        8        CHI   \n",
      "11   [{'name': 'Ian Arawjo', 'institutions': 'Corne...        8        CHI   \n",
      "12   [{'name': 'Franca Garzotto', 'institutions': '...        9        IDC   \n",
      "13   [{'name': 'Vivek K. Singh', 'institutions': 'R...        8       CSCW   \n",
      "14   [{'name': 'Tilman Dingler', 'institutions': 'U...        8  MOBILECHI   \n",
      "15   [{'name': 'Geza Kovacs', 'institutions': 'Stan...       14         LS   \n",
      "16   [{'name': 'David A. Joyner', 'institutions': '...        0         LS   \n",
      "17   [{'name': 'Joseph Jay Williams', 'institutions...       19         LS   \n",
      "18   [{'name': 'Scott Crossley', 'institutions': 'G...       12        LAK   \n",
      "19   [{'name': 'Susan Bull', 'institutions': 'Unive...       12        LAK   \n",
      "20   [{'name': 'Hendrik Drachsler', 'institutions':...       17        LAK   \n",
      "21   [{'name': 'Aneesha Bakharia', 'institutions': ...       11        LAK   \n",
      "..                                                 ...      ...        ...   \n",
      "129  [{'name': 'Ahmed Kharrufa', 'institutions': 'N...       35        ITS   \n",
      "130  [{'name': 'Cristina Conati', 'institutions': '...       41        IUI   \n",
      "131  [{'name': 'Ryan Shaun Baker', 'institutions': ...       98        CHI   \n",
      "132  [{'name': 'Y. Rogers', 'institutions': 'Indian...       95        IDC   \n",
      "133  [{'name': 'Andrés Moreno', 'institutions': 'Un...       82        AVI   \n",
      "134  [{'name': 'Jason A. Brotherton', 'institutions...       68      TOCHI   \n",
      "135  [{'name': 'Harry Brignull', 'institutions': 'U...       52       CSCW   \n",
      "118  [{'name': 'Matthew Kam', 'institutions': 'Univ...       34        DIS   \n",
      "119  [{'name': 'Maria F. Costabile', 'institutions'...       36        CHI   \n",
      "120  [{'name': 'Yu-Chen Chang', 'institutions': 'Na...       31        CHI   \n",
      "121  [{'name': 'Gillian R. Hayes', 'institutions': ...       36        CHI   \n",
      "122  [{'name': 'Alissa N. Antle', 'institutions': '...       38        IDC   \n",
      "160  [{'name': 'Michael Scaife', 'institutions': 'S...      112        CHI   \n",
      "161  [{'name': 'James C. Lester', 'institutions': '...      204        CHI   \n",
      "162  [{'name': 'Allison Druin', 'institutions': 'Co...       79        CHI   \n",
      "163  [{'name': 'Kori Inkpen', 'institutions': ['Uni...       85         GI   \n",
      "150  [{'name': 'Matthew Kam', 'institutions': 'Univ...       60        CHI   \n",
      "151  [{'name': 'Oren Zuckerman', 'institutions': 'M...      133        CHI   \n",
      "152  [{'name': 'Albert T. Corbett', 'institutions':...       49        CHI   \n",
      "136  [{'name': 'Ken Reily', 'institutions': 'Univer...       28      GROUP   \n",
      "137  [{'name': 'Michael S. Horn', 'institutions': '...       68        CHI   \n",
      "138  [{'name': 'Anne Marie Piper', 'institutions': ...       52        CHI   \n",
      "139  [{'name': 'Jochen Rick', 'institutions': 'Open...       54        IDC   \n",
      "153  [{'name': 'Zachary O. Toups', 'institutions': ...       37        CHI   \n",
      "154  [{'name': 'Nicolas Ducheneaut', 'institutions'...       78        CHI   \n",
      "155  [{'name': 'Ryan S.J.d. Baker', 'institutions':...       39        CHI   \n",
      "156  [{'name': 'Caitlin Kelleher', 'institutions': ...      147        CHI   \n",
      "157  [{'name': 'Udai Singh Pawar', 'institutions': ...       45        CHI   \n",
      "158  [{'name': 'Mitchel Resnick', 'institutions': '...      137        CHI   \n",
      "159  [{'name': 'Gregory D. Abowd', 'institutions': ...       62        CHI   \n",
      "\n",
      "     year     id  \n",
      "142  2018   7973  \n",
      "143  2018   8169  \n",
      "144  2018   8181  \n",
      "145  2018   8249  \n",
      "146  2018   8290  \n",
      "147  2018   8457  \n",
      "148  2018  15169  \n",
      "149  2018  21694  \n",
      "0    2017     67  \n",
      "1    2017     68  \n",
      "2    2017     74  \n",
      "3    2017    439  \n",
      "4    2017    445  \n",
      "5    2017    464  \n",
      "6    2017   1732  \n",
      "7    2017   9904  \n",
      "8    2017  10035  \n",
      "9    2017  10049  \n",
      "10   2017  10134  \n",
      "11   2017  10308  \n",
      "12   2017  15549  \n",
      "13   2017  18722  \n",
      "14   2017  20368  \n",
      "15   2016    134  \n",
      "16   2016    149  \n",
      "17   2016    212  \n",
      "18   2016    816  \n",
      "19   2016    819  \n",
      "20   2016    826  \n",
      "21   2016    856  \n",
      "..    ...    ...  \n",
      "129  2010  22890  \n",
      "130  2004   4183  \n",
      "131  2004   9013  \n",
      "132  2004  15346  \n",
      "133  2004  16532  \n",
      "134  2004  17291  \n",
      "135  2004  18433  \n",
      "118  2008   1796  \n",
      "119  2008  10790  \n",
      "120  2008  10818  \n",
      "121  2008  10855  \n",
      "122  2008  15728  \n",
      "160  1997  12687  \n",
      "161  1997  12689  \n",
      "162  1997  12702  \n",
      "163  1997  14890  \n",
      "150  2005   8925  \n",
      "151  2005   8958  \n",
      "152  2001   9157  \n",
      "136  2009   5413  \n",
      "137  2009  10606  \n",
      "138  2009  10632  \n",
      "139  2009  15616  \n",
      "153  2007  11071  \n",
      "154  2007  11083  \n",
      "155  2007  11110  \n",
      "156  2007  11155  \n",
      "157  2007  11170  \n",
      "158  1998  11208  \n",
      "159  1998  11228  \n",
      "\n",
      "[164 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for s in total_article:\n",
    "    for i in range(len(total_article[s]['title'])):\n",
    "        temp = []\n",
    "        title = total_article[s]['title'][i]\n",
    "        abstract = total_article[s]['abstract'][i]\n",
    "        author = author_data[s][i]\n",
    "        index = total_id[s][i]\n",
    "        conf = total_conf[s][i]\n",
    "        citation = total_bibliometrics[s][i]\n",
    "        num = article_position[title]\n",
    "        \n",
    "        temp.append(num)\n",
    "        temp.append(title)\n",
    "        temp.append(abstract)\n",
    "        temp.append(author)\n",
    "        temp.append(citation)\n",
    "        temp.append(conf)\n",
    "        temp.append(s)\n",
    "        temp.append(index)\n",
    "        \n",
    "        \n",
    "        df1.loc[num] = temp\n",
    "        \n",
    "        num += 1\n",
    "        \n",
    "print (df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T02:12:50.657437Z",
     "start_time": "2019-12-27T02:12:50.574671Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/home/lr/Downloads/conference_data/education_high_cited/\"\n",
    "if not os.path.exists(path):\n",
    "     os.mkdir(path)\n",
    "#date_rand =time.strftime(\"%Y-%m-%d\",time.localtime())\n",
    "file_name = path+'artical_info.xlsx'\n",
    "writer = pd.ExcelWriter(file_name)\n",
    "\n",
    "df1.to_excel(writer, \"article_info\")\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类分组+词云"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T01:57:33.785032Z",
     "start_time": "2019-12-27T01:57:33.777298Z"
    }
   },
   "outputs": [],
   "source": [
    "onlineLearning = [62,90,61,96,23,83,63,82,72,47,147,45,49,50,54,15,5,69,68,77,59,146,31,159,134]\n",
    "tabletopInterface = [117,98,108,116,129,139,138]\n",
    "STEMedu = [1,34,40,52,94,11,92,24,143,137,156,64,76,122,6,142,151,158,140,125]\n",
    "stuLife = [84,65]\n",
    "onlineEngag = [87,36,74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWC(group):\n",
    "    word_sum = []\n",
    "    for s in group:\n",
    "        position = findPosition(s)\n",
    "        year = position[0]\n",
    "        index = position[1]\n",
    "\n",
    "        title = total_article[year]['title'][index]\n",
    "        abstract = total_article[year]['title'][index]\n",
    "        text = title + ' ' + title + ' ' + abstract\n",
    "\n",
    "        sent = splitSentence(title) \n",
    "        word_list = []\n",
    "        for m in sent:\n",
    "            word = wordtokenizer(m)                  \n",
    "            word_list = word_list + word\n",
    "\n",
    "        word_standard = standardization(word_list)\n",
    "        word_ori = lemmatizer(word_standard)\n",
    "\n",
    "        word_sum.extend(word_ori)\n",
    "\n",
    "    #print (word_sum)\n",
    "    prep_list = '/'.join(word_sum)\n",
    "    #print (prep_list)\n",
    "    \n",
    "    from wordcloud import WordCloud\n",
    "    wc = WordCloud(width = 600, height = 300).generate(prep_list)\n",
    "    image = wc.to_image()\n",
    "    image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T02:23:00.925644Z",
     "start_time": "2019-12-27T02:23:00.863806Z"
    }
   },
   "outputs": [],
   "source": [
    "def wordList(group):\n",
    "    word_sum = []\n",
    "    for s in group:\n",
    "        info = df1.loc[s]\n",
    "        title = info['title']\n",
    "        abstract = info['abstract']\n",
    "\n",
    "        text = title + ' ' + title + ' ' + abstract\n",
    "\n",
    "        sent = splitSentence(title) \n",
    "        word_list = []\n",
    "        for m in sent:\n",
    "            word = wordtokenizer(m)                  \n",
    "            word_list = word_list + word\n",
    "\n",
    "        word_standard = standardization(word_list)\n",
    "        word_ori = lemmatizer(word_standard)\n",
    "\n",
    "        word_sum.extend(word_ori)\n",
    "\n",
    "    prep_list = '/'.join(word_sum)\n",
    "    return prep_list\n",
    "\n",
    "ol_list = wordList(onlineLearning)\n",
    "ti_list = wordList(tabletopInterface)\n",
    "stem_list = wordList(STEMedu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T02:34:02.576719Z",
     "start_time": "2019-12-27T02:34:00.611916Z"
    }
   },
   "outputs": [],
   "source": [
    "def generateWC(prep_list,string):\n",
    "    from wordcloud import WordCloud\n",
    "    wc = WordCloud(width = 800, height = 600).generate(prep_list)\n",
    "    image = wc.to_image()\n",
    "    wc.to_file(string)\n",
    "    image.show()\n",
    "    \n",
    "generateWC(ol_list,'OnlingLearning.png')\n",
    "generateWC(ti_list,'tabletopInterface.png')\n",
    "generateWC(stem_list,'STEM.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T09:56:17.781715Z",
     "start_time": "2019-12-27T09:56:17.770946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "6\n",
      "24\n",
      "29\n",
      "34\n",
      "36\n",
      "39\n",
      "54\n",
      "55\n",
      "59\n",
      "60\n",
      "63\n",
      "71\n",
      "77\n",
      "81\n",
      "82\n",
      "89\n",
      "91\n",
      "97\n",
      "102\n",
      "103\n",
      "105\n",
      "111\n",
      "131\n",
      "132\n",
      "133\n",
      "137\n",
      "145\n",
      "149\n",
      "154\n",
      "158\n"
     ]
    }
   ],
   "source": [
    "for s in range(len(new_cite_list)):\n",
    "    if new_cite_list[s] < 30 and new_cite_list[s] > 20:\n",
    "        print (s)\n",
    "        continue\n",
    "    \"\"\"if new_cite_list[s] > 30:\n",
    "        print (str(s) + \"{color:\" + color[1] + \"}\")\n",
    "        continue\n",
    "    if new_cite_list[s] > 20:\n",
    "        print (str(s) + \"{color:\" + color[0] + \"}\")\n",
    "        continue\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
